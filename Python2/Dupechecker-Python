#!/usr/bin/python2.7
# dupechecker
# Finds duplicate files based on md5
# and filesize

import sys, getopt, os, hashlib

## Defaults
recursive   = False       #defaults to not recursive
directory   = "./"        #default is current dir
ask         = True        #defaults to requiring confirmation

def md5_for_file(filename, block_size=2**20):
    f = open(filename)
    md5 = hashlib.md5()
    while True:
        data = f.read(block_size)
        if not data:
            break
        md5.update(data)
    return md5.digest()

class fileblock():
    def __init__(self, inpath):
        self.wholepath = inpath
        self.size = os.path.getsize(inpath)
        self.md5 = ""


## Rip out any genuine paths in the args
for arg in sys.argv:
    if os.path.isdir(arg):
        directory = arg

## Handle Args
try:
    opts, args = getopt.gnu_getopt( sys.argv, "ry", ["recursive", "yes-to-all"] )

except getopt.GetoptError:
    print str(err)
    usage()

for o, a in opts:
    if o in ("-r", "--recursive"):
        recursive = True
    elif o in ("-y", "--yes-to-all"):
        ask = False

## Check that we've got a valid filepath
if os.path.isdir(directory) == False:
    print "Error! " + directory + " is not a valid directory! Exiting."
    sys.exit(' ')

## Lists used for the hash checking
filestats = []

##MAIN
#Get a list of files only (no dirs)
for tempfile in os.listdir(directory):
    if os.path.isfile(directory + tempfile):
        filestats.append( fileblock( directory + tempfile ) )


for tempfile in filestats:
    print "File: " + tempfile.wholepath
    print "Size: " + str(tempfile.size)

